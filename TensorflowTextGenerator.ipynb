{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Text Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to create a recursive \n",
    "neural network based on LSTM cells to generate random text based on raw input from a novel, in this case the early 17th century novel Don Quixote, by Miguel de Cervantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textfile=open('quijote.txt')\n",
    "a=textfile.read()\n",
    "\n",
    "#Removing useless characters\n",
    "a=a.replace('\\n',' ').replace('\\r','').replace('  ',' ').replace('  ',' ').replace('  ',' ').replace('  ',' ')\n",
    "a=np.array(list(a))\n",
    "\n",
    "#Finding unique characters to define our dictionary\n",
    "alphabet=np.unique(a)\n",
    "\n",
    "#Replacing each character by its index in the dictionary\n",
    "aind=np.zeros(a.shape,dtype=np.int32)\n",
    "for i in range(len(alphabet)):\n",
    "    aind[a==alphabet[i]]=i\n",
    "\n",
    "#Since this will be a recursive neural network, the output will always be the next character, to the one used as input\n",
    "training_inputs=aind[0:-1]\n",
    "training_outputs=aind[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Start an interactive session, more convenient for multicell execution in the notebook\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "#Some parameters\n",
    "num_hidden=256\n",
    "num_layers=2\n",
    "data_type=np.float32\n",
    "max_grad_norm = 5\n",
    "\n",
    "init_scale = 0.1   #Random initialization scale\n",
    "vocab_size=len(alphabet) #Number of possible numbers: the length of the one hot representation\n",
    "\n",
    "\n",
    "#Place holders for inputws, outputs, and other config values\n",
    "input_data = tf.placeholder(tf.int32, [None, None]) #Placeholder for the input data: It will be: Nbatch,Ntimesteps(,1 input variable)\n",
    "targets = tf.placeholder(tf.int32, [None, None])  #Equivalent placeholder for the targets\n",
    "\n",
    "batch_size = tf.placeholder(tf.int32)  #Placeholder for the batch_size, we will use different values training, and predicting\n",
    "keep_prob = tf.placeholder(tf.float32) #Placeholder for the keeping probability in the droput technique\n",
    "\n",
    "\n",
    "#Structure of LSTM cells\n",
    "cellLSTM = tf.nn.rnn_cell.BasicLSTMCell(num_hidden, forget_bias=0.0, state_is_tuple=True) #Properties of the single LSTM layer\n",
    "cellLSTM_drop = tf.nn.rnn_cell.DropoutWrapper(cellLSTM, output_keep_prob=keep_prob)\n",
    "cellsLSTM = tf.nn.rnn_cell.MultiRNNCell([cellLSTM_drop] * num_layers, state_is_tuple=True)\n",
    "\n",
    "initial_state = cellsLSTM.zero_state(batch_size, data_type)         #Initial state for the cells, for as many batches as required\n",
    "\n",
    "\n",
    "inputs_cell = tf.one_hot(input_data,vocab_size)    #We convert to 1-hot representation before feeding the values\n",
    "outputs, state = tf.nn.dynamic_rnn(cellsLSTM, inputs_cell, initial_state=initial_state,time_major = False) #Time_major=False, as the first dimension is the batch nubmer\n",
    "\n",
    "output = tf.reshape(outputs, [-1, num_hidden])       #To calculate the error of the join the Nbatch,Ntimesteps axes together \n",
    "\n",
    "#We apply a final softmax layer, with the corresponding weights, and calculate the cost and cross entropy with the target values\n",
    "softmax_w = tf.get_variable(\"softmax_w\", [num_hidden, vocab_size], dtype=data_type)  \n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=data_type)\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits, tf.one_hot( tf.reshape(targets, [-1]),vocab_size))\n",
    "cost = tf.reduce_sum(loss) / tf.cast(batch_size,np.float32)\n",
    "\n",
    "#We store the final state of the cell in a tensor\n",
    "final_state = state\n",
    "\n",
    "\n",
    "#And the gradient descent part for training\n",
    "lr = tf.Variable(0.0, trainable=False)   #Learning rate\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),max_grad_norm)\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "new_lr = tf.placeholder(tf.float32, name=\"new_learning_rate\") #To add the posibility to change the learning rate\n",
    "lr_update = tf.assign(lr, new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the variables, and loading if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "#We initialize all the variables\n",
    "initializer = tf.random_uniform_initializer(-init_scale,init_scale)\n",
    "tf.initialize_all_variables().run()\n",
    "saver=tf.train.Saver(tf.all_variables())\n",
    "name='rnn-model'\n",
    "\n",
    "\n",
    "imaster=0\n",
    "#iload=None\n",
    "iload=4896\n",
    "restoreFile='rnn-model-%i'%iload\n",
    "\n",
    "if restoreFile:\n",
    "    saver.restore(session,'%s-%i'%(name,iload))\n",
    "    imaster=iload\n",
    "    print('Loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to generate random text\n",
    "It will keep generating characters, form an initial seed until it find a full-stop within a certain limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GenerateText(start='En',minlength=50,maxlength=400):\n",
    "    \n",
    "    start=np.array(list(start))\n",
    "    startind=np.zeros(start.shape,dtype=np.int32)\n",
    "   \n",
    "    for i in range(len(alphabet)):\n",
    "        startind[start==alphabet[i]]=i\n",
    "    #Now we predict the ouputs for a given input \n",
    "\n",
    "    #Some functions to calculate softmax, and pick a value\n",
    "    def softmax(probs,t):\n",
    "        probsout=np.exp(probs/t)\n",
    "        return (probsout.T/np.sum(probsout,axis=-1)).T\n",
    "\n",
    "    def selectHigh(probs):\n",
    "        return np.argmax(probs,axis=-1)\n",
    "\n",
    "    def selectRandomHigh(probs):\n",
    "        return np.random.choice(np.arange(len(probs)), p=probs)\n",
    "\n",
    "    temperature=1 # For softmax\n",
    "    \n",
    "    #We will chose the next character by doign a weighted random sampling of the softmax output\n",
    "    selectionFunction=selectRandomHigh\n",
    "    val=startind[0]\n",
    "\n",
    "\n",
    "    #Initial state of the LSTMs\n",
    "    state_predict= session.run(initial_state,{batch_size:1})\n",
    "\n",
    "    output=[]\n",
    "    string=''\n",
    "    array=np.zeros((1,1),np.int16)\n",
    "    for i in range(maxlength):\n",
    "        string+=alphabet[val]\n",
    "        \n",
    "        if len(string)>minlength and string[-1]=='.':\n",
    "            break\n",
    "        \n",
    "        array[0,0]=val\n",
    "        fetches = [logits,final_state] # Now we need the output and the final_state to forward it to the next iteration\n",
    "        feed_dict = {batch_size:1,input_data:array,keep_prob:1} #We are now predicting character by character without dropout\n",
    "\n",
    "        #We need to carry the state of the LSTM towards to next iteration\n",
    "        for j, (c, h) in enumerate(initial_state):\n",
    "            feed_dict[c] = state_predict[j].c\n",
    "            feed_dict[h] = state_predict[j].h\n",
    "\n",
    "\n",
    "        logits_a,state_predict= session.run(fetches, feed_dict)\n",
    "\n",
    "        if (i+1)<len(startind):\n",
    "            val=startind[i+1]\n",
    "        else:\n",
    "            val=selectionFunction(softmax(logits_a[0,:],temperature))\n",
    "            \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Las de Caldo, miradosa confis\\xf3n que no pudo sufer que estas cruelles de Olacio, y apenas le hab\\xeda de pasar esta batalla, dijo: cuya lugar que \\xe9l este (que eran carneras a los \\xe1rboles, o que le pagaba, solo las diante a este rey y dejando de buena perdici\\xf3n y determinaro a poso, no los serv\\xedan, le dijo: pues en poseci\\xf3n, conquitan los fuercas y caballeros, tan caresan de cuatro coranis del nombre d'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerateText('La')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Epoch: 4898 Learning rate: 0.000'\n",
      "'0.045 perplexity: 2.047 speed: 1507 cps'\n",
      "'0.145 perplexity: 2.064 speed: 1871 cps'\n",
      "'0.245 perplexity: 2.058 speed: 1964 cps'\n",
      "'0.345 perplexity: 2.043 speed: 2002 cps'\n",
      "'0.445 perplexity: 2.040 speed: 2029 cps'\n",
      "'0.545 perplexity: 2.035 speed: 2046 cps'\n",
      "'0.645 perplexity: 2.034 speed: 2056 cps'\n",
      "'0.745 perplexity: 2.037 speed: 2064 cps'\n",
      "'0.845 perplexity: 2.033 speed: 2071 cps'\n",
      "'0.945 perplexity: 2.032 speed: 2072 cps'\n",
      "'0.995 perplexity: 2.030 speed: 2068 cps'\n",
      "'Sample text: En mi provecho; y le di\\xf3 tres delos caballeros pensaba decar esto que su amo nos ha piese y que para con otra cosa que no se ha de poner grande y pierra en el cabrero.'\n",
      "'Epoch: 4899 Learning rate: 0.000'\n",
      "'0.045 perplexity: 2.036 speed: 1289 cps'\n",
      "'0.145 perplexity: 2.060 speed: 1709 cps'\n",
      "'0.245 perplexity: 2.071 speed: 1837 cps'\n"
     ]
    }
   ],
   "source": [
    "verbose=True\n",
    "\n",
    "num_steps_training=70  #Number of timesteps\n",
    "batch_size_training=20 #Number of batches per loop\n",
    "keep_prob_training=0.5 #Dropout probability\n",
    "\n",
    "learning_rate = 1.0    #Starting learning rate\n",
    "lr_decay_cfg = 0.998   #Decay of the learning rate\n",
    "\n",
    "\n",
    "#Reshaping the data into batches\n",
    "crop=len(training_inputs)//batch_size_training*batch_size_training\n",
    "training_inputs_batch=training_inputs[:crop].reshape(batch_size_training,-1)\n",
    "training_outputs_batch=training_outputs[:crop].reshape(batch_size_training,-1)\n",
    "\n",
    "def LogFile(string,path='log.txt'):\n",
    "    filelog=open('log.txt','a+')\n",
    "    print(repr(string))\n",
    "    filelog.write('%s\\n'%string)\n",
    "    filelog.close()\n",
    "\n",
    "#In a try-catch block, to stop it by stopping the kernel    \n",
    "try:\n",
    "    im=imaster+1\n",
    "    while True:\n",
    "        #Update the learning rate based on the decay\n",
    "        lr_decay = lr_decay_cfg  ** max(im - 4, 0.0)\n",
    "        session.run(lr_update, feed_dict={new_lr: learning_rate * lr_decay})\n",
    "\n",
    "        LogFile(\"Epoch: %d Learning rate: %.3f\" % (im , session.run(lr)))\n",
    "\n",
    "        epoch_size = training_inputs_batch.shape[1]// num_steps_training  #Number of iterations we will have to run per epoch\n",
    "        start_time = time.time()\n",
    "        costs = 0.0\n",
    "        iters = 0\n",
    "\n",
    "        #We need an initial state\n",
    "        state = session.run(initial_state,{batch_size:batch_size_training})\n",
    "        for step in range(epoch_size):\n",
    "            #We need:\n",
    "                #cost: to check that it goes down\n",
    "                #final_state, to forward to the next iteration\n",
    "                #train_op: A training operation\n",
    "            fetches = [cost, final_state, train_op]\n",
    "            feed_dict = {}\n",
    "\n",
    "            #feeding the number of batches\n",
    "            feed_dict[batch_size]=batch_size_training\n",
    "            feed_dict[keep_prob] = keep_prob_training\n",
    "\n",
    "            #Feeding the right inputs and outputs\n",
    "            ind1=step*(num_steps_training)\n",
    "            ind2=(step+1)*(num_steps_training)\n",
    "            feed_dict[input_data] = training_inputs_batch[:,ind1:ind2]\n",
    "            feed_dict[targets] = training_outputs_batch[:,ind1:ind2]\n",
    "\n",
    "            #Feeding the initial state\n",
    "            for i, (c, h) in enumerate(initial_state):\n",
    "                feed_dict[c] = state[i].c\n",
    "                feed_dict[h] = state[i].h\n",
    "\n",
    "            #Running the session\n",
    "            cost_a, state, _ = session.run(fetches, feed_dict)\n",
    "\n",
    "            costs += cost_a\n",
    "            iters += num_steps_training\n",
    "\n",
    "            if verbose and step % (epoch_size // 10) == 10:\n",
    "                LogFile(\"%.3f perplexity: %.3f speed: %.0f cps\" %(step * 1.0 / epoch_size, np.exp(costs / iters),iters * batch_size_training / (time.time() - start_time)))\n",
    "\n",
    "        train_perplexity=np.exp(costs / iters)\n",
    "        imaster=im\n",
    "\n",
    "        saver.save(session, name, global_step=imaster)\n",
    "\n",
    "        LogFile(\"%.3f perplexity: %.3f speed: %.0f cps\" %(step * 1.0 / epoch_size, np.exp(costs / iters),iters * batch_size_training / (time.time() - start_time)))\n",
    "\n",
    "        text=GenerateText()\n",
    "        LogFile(\"Sample text: %s\" %text)\n",
    "        im+=1\n",
    "except:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
